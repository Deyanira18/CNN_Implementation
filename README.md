# CNN_Implementation
Implementation of custom layer in a Convolutional Neural Network on an image dataset.
This work has been finished by three people.

# Implementation of the Layers
Custom Layer Description:
a. Custom Layer Class: A custom layer class is being created by inheriting from the Keras Layer class. This custom class will define how the layer will be computed during the training process.
 b. InitializingtheFiltersProperty:Theclassconstructorinitializesthefiltersproperty to store the number of filters or weight matrices to be used for the current layer. This filter property will be used in the Custom Layer constructor when being used in the model.
c. Initializing Weights: The build() function is called during the model building step, where the weights are initialized using the add_weight() function. The initializer argument is set to "random_normal" to randomly initialize the weights. Additionally, the trainable parameter is set to True to ensure that the weight is updated during training.
d. Defining the Layer Computation Steps: The call() function of the class is defined to perform the layer computation. The function calculates the feature map for each weight matrix in the layer.
e. Looping Through Samples and Feature Maps: The function loops through all the samples and feature maps to calculate the weighted sum of the receptive field using a 3x3 kernel. The receptive field is the area that is considered when applying the filter to the input.
f. Sparse Tensor: After calculating the weighted sum, a sparse tensor is used to generate a matrix with the feature map value to be set into the output map. The sparse tensor is used to efficiently handle large data sets.
g. Updating the Output Map: The output map is then updated with the feature map pixel value for the current receptive field. This process continues until all the receptive fields have been processed.
h. Reshaping the Output Map: Finally, the output map is reshaped to maintain its shape during training. This ensures that the output from the custom layer can be passed on to the next layer in the model without any shape mismatch errors.


# Custom Pooling Layer Description:
a. The code creates a custom pooling layer class by inheriting from the Keras Layer class. In Keras, layers are the basic building blocks of a deep neural network. The CustomPoolingLayers is a custom implementation of the pooling layer that can be used in place of the default pooling layer provided by Keras.
b. The init function initializes the CustomPoolingLayers by calling the init function of the parent class (i.e., the Keras Layer class).
c. The call function defines the max pooling computation to be performed on each feature map. In this function, the input data is passed through the pooling layer to produce the output.
d. First, the output size of the pooling operation is initialized to half of the original input size.
e. Then, a tensor of zeros is initialized with the same shape as the expected output shape of the pooling operation. This tensor will be filled with the actual pooled values later.
f. The range of indices with a stride size of 2 is initialized to be traversed to get the 2x2 sub-matrix on which pooling is performed. This is done using the tf.range function.
g. The code checks if the input feature map has odd or even dimensions, ignoring the last column and row if the shape is odd. This is done by checking if the end value of the range of indices is odd or even.
h. The axes values for matrix traversal are set based on the shape of the pooled_maps tensor.
i. For each sample and each feature map, the max pooling operation is performed on the 2x2 sub-matrix by using the tf.reduce_max function.
j. The pooled output for each feature map is saved in a tensor.
k. Thepooledoutputforeachsampleinthedatasetissavedinatensor.
l. Finally, the pooled_maps tensor is reshaped and returned in a tensorflow- compatible form. The output shape is consistent with the expected output shape of the pooling operation.


# The Complete Neural Network Model Description:
a. The neural network outlined below has three proposed layers, two max pooling layers, and adds the Keras Flatten layer to reduce the inputs from the two dimensions to a single dimension vector. However, due to computational constraints, the weight matrices used for each of the three layers are 5, 3, and 2 respectively, rather than 16, 12, or 8.
b. The system can successfully train the model with an image size of 28x28; however, going beyond that requires more than two hours for one epoch.
c. The function initializes a sequential model using the Sequential class from Keras.
d. Acustomlayerwith5weightmatricesisaddedtogenerate5featuremaps.
e. TheReLUactivationfunctionisaddedtointroducenon-linearitytothemodel.
f. A custom pooling layer is added to perform max pooling on the feature maps
generated by the previous layer.
g. Thisisrepeatedwith3and2featuremaps.
 h. The neural network was tested with 16, 12, and 8 layers, but the system's constrained resources prevented it from completing the training process.
i. Overall, the model architecture consists of multiple custom layers that generate feature maps, ReLU activation functions to introduce non-linearity, custom pooling layers to perform max pooling, and fully connected layers to combine the features. The output layer uses the softmax activation function to predict the class probabilities.


# Implementation and Results
The stages are defined in the method model run that follows.
a. ThemethodModelrun()isusedtocompile,train,andevaluatetheneuralnetwork model.
b. It takes in three parameters: alpha (learning rate), batch_size (number of training samples used in one iteration), and epoch (number of times the training data is passed through the neural network model).
c. A dictionary run_result is initialized to store the learning rate, batch size, epoch, training accuracy, and test accuracy.
d. The adam parameter is a boolean value which, when set to True, uses the Adam optimizer for training the neural network model. Otherwise, it uses the SGD optimizer.
e. The appropriate optimizer is chosen and compiled with the model using the compile() method. The loss function used for optimization is BinaryCrossentropy, and the accuracy metric is used to evaluate the model.

 f. A directory is initialized and set up to store the run files for TensorBoard, a tool for visualizing training data.
g. The fit() method is used to train the model with the given parameters. The X_train and y_train variables represent the input and target output of the training dataset, respectively. The X_valid and y_valid variables represent the input and target output of the validation dataset, respectively. The batch_size and epoch parameters determine the number of training samples used in one iteration and the number of iterations to train the model, respectively. The tensorboardcallbacks is used to save the training progress logs to the directory set up earlier.
h. The training accuracy is saved by extracting the accuracy data from the history object returned by the fit() method.
i. The evaluate() method is used to evaluate the trained model on the test dataset, represented by the X_test and y_test variables.
j. The training accuracy and test accuracy are formatted as percentages and stored in the run_result dictionary.
k. Finally,themodel_historyobjectandrun_resultdictionaryarereturned.

# The results of the run are being saved to test the implementation of the layer and the model.
With the default parameter settings (batch size None and Learning Rate 0.01) of the Stochastic Gradient Descent optimizer offered by Keras Optimizer class, the model generated using the build model method was trained with all data samples using the model run method as follows:

histories.append(model_history) results.append(run_result)
print("For the default settings of Stochastic Gradient Descent Optimizer the designed model has following accuracies:") print(f"Training Accuracy: {run_result['Train Accuracy']}") print(f"Test Accuracy: {run_result['Test Accuracy']}")
The Stochastic Gradient Descent Optimizer for the model has following accuracies: Training Accuracy: 53.125%
Test Accuracy: 62.5%
 
# Data Pre-Processing and Data-Set splitting into train, test and validation
Description:
a. Settingdirectorynamesandcategories:
 i. DataDirectory variable is set to the directory path where the images are stored.
  
 ii. Categories variable is a list of categories for the images, which in this case are "alpaca" and "not alpaca".
b. Loadinganddisplayinganimage:
 i. A loop is initiated to iterate through each category in Categories.
 ii. os.path.join() is used to create a path to the directory for each category.
 iii. A loop is then initiated to iterate through each image in the category directory.
 iv. cv2.imread() is used to read the image file into an array and convert it to grayscale.
 v. plt.imshow() is used to display the grayscale image.
 vi. plt.show() is used to display the image in a pop-up window.
 vii. The loop is then broken after the first image is displayed.
c. Resizing an image:
 i. The variable Img_Size is set to 28, which is the desired size for the images.
 ii. cv2.resize() is used to resize the image to the desired size.
 iii. plt.imshow() is used to display the resized image.
 iv. plt.show() is used to display the image in a pop-up window.
d. Creatingtrainingdata:
 i. A function called create_TrainData() is defined to load and preprocess all the images in the directories.
 ii. A loop is initiated to iterate through each category in Categories.
 iii. os.path.join() is used to create a path to the directory for each category.
 iv. Categories.index() is used to assign a number to the class based on its index (0 for "alpaca" and 1 for "not alpaca").
 v. Another loop is initiated to iterate through each image in the category directory.
 vi. cv2.imread() is used to read the image file into an array and convert it to grayscale.
 vii. cv2.resize() is used to resize the image to the desired size.
 viii. The resized image array and its class number are appended to a list called Training_Data.
 ix. If an exception is raised due to a distorted image, the loop is continued to the next image.
 x. random.shuffle() is used to randomly shuffle the Training_Data list.
e. Splittingdataintofeaturesandlabels:
 i. Two empty lists, X and y, are defined to hold the features and labels respectively.
 ii. A loop is initiated to iterate through each feature-label pair in the Training_Data list.
 iii. The feature array is appended to the X list, and the label is appended to the y list.
 iv. np.array() is used to convert X and y to numpy arrays.

 CODE:
f. Preparing data for CNN model:
i. np.copy() is used to create a copy of the X numpy array.
ii. np.array().reshape() is used to reshape the X numpy array into a format suitable for the CNN model.
iii. tf.cast() is used to cast the X numpy array to a format supported by TensorFlow.
g. Splittingdataintotraining,validation,andtestingsets:
i. The size of the dataset is obtained using len().
ii. The total size of the validation and testing sets is calculated as one- third of the dataset size.
iii. The size of the validation set is calculated as half of the total validation and testing set size.
iv. The X_train and y_train arrays are created by slicing the X and y numpy arrays up to the size of the training set.



# Outcomes using different Hyperparameters
Description:
a. To assess the performance of the model developed in task 3, various hyperparameters were trained on it.
b. Althoughthenumberofepochshadtoberestrictedto5and7,duetotheCustom Layer's computational complexity, the model was trained and assessed using a variety of optimizer configurations, learning rates, batch sizes, and learning rates to test for differences in convergence.
c. The Keras Optimizer class's Stochastic Gradient Descent and Adam optimizers are employed.



